{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Read Fashion MNIST dataset\n",
    "\n",
    "import util_mnist_reader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pandas import DataFrame as df\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing, metrics\n",
    "from matplotlib import pyplot\n",
    "# from sklearn import s\n",
    "X_train, y_train = util_mnist_reader.load_mnist('data/fashion', kind='train')\n",
    "X_test, y_test = util_mnist_reader.load_mnist('data/fashion', kind='t10k')\n",
    "\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes=10)# one-hot vectors\n",
    "y_test = to_categorical(y_test, num_classes=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoid(Z):\n",
    "    return (1/ (1+np.exp(-Z)))\n",
    "\n",
    "def Softmax(Z):\n",
    "    den = (np.sum(np.exp(Z), axis = 1))\n",
    "    den = np.matrix(den.transpose())\n",
    "    num = (np.exp(Z)).transpose()\n",
    "    return (num/den).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t 2.2629147532097447\n",
      "1 \t 2.197718830717796\n",
      "2 \t 2.140217184651838\n",
      "3 \t 2.084779595851707\n",
      "4 \t 2.0311113462978616\n",
      "5 \t 1.97912081377\n",
      "6 \t 1.9287979849295613\n",
      "7 \t 1.8801777768091907\n",
      "8 \t 1.8333128987580394\n",
      "9 \t 1.7882534744955292\n",
      "10 \t 1.745034906538107\n",
      "11 \t 1.7036721188388115\n",
      "12 \t 1.6641578206928305\n",
      "13 \t 1.6264634766511412\n",
      "14 \t 1.5905424351571644\n",
      "15 \t 1.556334424133034\n",
      "16 \t 1.5237702703817542\n",
      "17 \t 1.492775884039651\n",
      "18 \t 1.4632751183886725\n",
      "19 \t 1.4351916141302123\n",
      "20 \t 1.408449950973057\n",
      "21 \t 1.3829764106677818\n",
      "22 \t 1.3586995421544823\n",
      "23 \t 1.3355506114428886\n",
      "24 \t 1.3134639546744384\n"
     ]
    }
   ],
   "source": [
    "#hyperparameters\n",
    "\n",
    "alpha = 0.35 # Learning Rate\n",
    "n = 100 # No. of neurons in 2nd layer\n",
    "epochs = 1500\n",
    "\n",
    "\n",
    "#Weights between input and hidden layer\n",
    "W1 = np.random.normal(scale=0.1, size=(X_test.shape[1],n))\n",
    "b1 = np.random.normal(scale=0.1, size=(n,1))\n",
    "#Weights between hidden and output layer\n",
    "W2 = np.random.normal(scale=0.1, size=(n,10))\n",
    "b2 = np.random.normal(scale=0.1, size=(10,1))\n",
    "\n",
    "loss_plot = list()\n",
    "val_loss_plot = list()\n",
    "\n",
    "\n",
    "A0 = X_train # Input layer\n",
    "Z1 = np.dot(A0,W1).transpose() + b1\n",
    "A1 = Sigmoid(Z1.transpose()) # Hidden layer\n",
    "Z2 = np.dot(A1,W2).transpose() + b2\n",
    "A2 = Softmax(Z2.transpose()) # output layer\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    dW2 = np.dot(A1.transpose(),(A2-y_train))/60000\n",
    "    dL_dZ2 = (A2-y_train)\n",
    "    dZ2_dA1 = W2\n",
    "\n",
    "\n",
    "    dL_dA1 = np.dot(dL_dZ2, dZ2_dA1.transpose()) #60k,100\n",
    "    dA1_dZ1 = np.multiply(A1,1-A1) #60k,100\n",
    "    dZ1_dW1 = X_train\n",
    "\n",
    "    dL_dZ1 = np.multiply(dL_dA1, dA1_dZ1)\n",
    "    dL_dW1 = np.dot(dZ1_dW1.transpose(),dL_dZ1)\n",
    "    dW1 = dL_dW1/60000\n",
    "    \n",
    "    db1 = np.sum(dL_dZ1,axis=0)/60000\n",
    "    db1 = db1.transpose()\n",
    "    db2 = np.sum(dL_dZ2,axis=0)/60000\n",
    "    db2 = db2.transpose()\n",
    "       \n",
    "    \n",
    "    W2 = W2 - (alpha * dW2)\n",
    "    W1 = W1 - (alpha * dW1)\n",
    "    \n",
    "    b1 = b1 - (alpha * db1)\n",
    "    b2 = b2 - (alpha * db2)\n",
    "    \n",
    "    #print (b1,b2)\n",
    "\n",
    "\n",
    "    Z1 = np.dot(A0,W1).transpose() + b1\n",
    "    A1 = Sigmoid(Z1.transpose()) # Hidden layer\n",
    "    Z2 = np.dot(A1,W2).transpose() + b2\n",
    "    A2 = Softmax(Z2.transpose()) # output layer\n",
    "    \n",
    "    loss = - np.sum(np.multiply(y_train,np.log(A2)))/60000\n",
    "    \n",
    "    \n",
    "    \n",
    "    pA0 = X_test # Input layer\n",
    "    pZ1 = np.dot(pA0,W1).transpose() + b1\n",
    "    pA1 = Sigmoid(pZ1.transpose()) # Hidden layer\n",
    "    pZ2 = np.dot(pA1,W2).transpose() + b2\n",
    "    pA2 = Softmax(pZ2.transpose()) # output layer\n",
    "    \n",
    "    val_loss = - np.sum(np.multiply(y_test, np.log(pA2)))/10000\n",
    "    print(epoch,\"\\t\",loss)\n",
    "    \n",
    "    loss_plot.append(loss)\n",
    "    val_loss_plot.append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pA0 = X_test # Input layer\n",
    "pZ1 = np.dot(pA0,W1).transpose() + b1\n",
    "pA1 = Sigmoid(pZ1.transpose()) # Hidden layer\n",
    "pZ2 = np.dot(pA1,W2).transpose() + b2\n",
    "pA2 = Softmax(pZ2.transpose()) # output layer\n",
    "\n",
    "\n",
    "actual = y_test\n",
    "prediction = pA2.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(actual,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plottable_image = np.reshape(X_test[2], (28, 28))\n",
    "\n",
    "# # Plot the image\n",
    "# plt.imshow(plottable_image, cmap='gray_r')\n",
    "\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
