{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jNnBupXE8XvZ"
   },
   "outputs": [],
   "source": [
    "# Import all gym related libraries\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "# Take action\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uTV2CFH18u0q"
   },
   "outputs": [],
   "source": [
    "# Define the Show state function\n",
    "env = gym.make('SpaceInvaders-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sGgJ13lM8_hh"
   },
   "outputs": [],
   "source": [
    "observation_space = env.observation_space\n",
    "action_space = env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5jACjPog9GSN"
   },
   "outputs": [],
   "source": [
    "# Print observation space is \n",
    "print(\"Observation Space is :\",observation_space)\n",
    "print(\"Number of actions is :\",action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nj8fSZBv-Vqq"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I5uzM0cx-bTc"
   },
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xj-Ur7-a9RW4"
   },
   "outputs": [],
   "source": [
    "# define libraries to pre-process image\n",
    "from skimage import transform\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fVl01AIf-gR4"
   },
   "outputs": [],
   "source": [
    "def process_frame(frame) :\n",
    "\n",
    "    # 1. convert image from rgb to gray\n",
    "    gray_image = rgb2gray(frame)\n",
    "\n",
    "    # 2. Crop the image\n",
    "    cropped_image = gray_image[0:-12,4:-12]\n",
    "\n",
    "    # 3. Normalized frame\n",
    "    normalized_frame = cropped_image / 255.0\n",
    "\n",
    "    # 4. Resize\n",
    "    resize_image = transform.resize(normalized_frame,[110,84])\n",
    "\n",
    "    return resize_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Aok4hP91_O58"
   },
   "outputs": [],
   "source": [
    "def create_frame_stack(frame,existing_stack,max_len=16) :\n",
    "\n",
    "    # 1. Create processed frame\n",
    "    processed_frame = process_frame(frame)\n",
    "\n",
    "    # 2. append it to the stack\n",
    "    existing_stack.append(processed_frame)\n",
    "\n",
    "    # 3. if length of stack exceeds 4 then remove the last one\n",
    "    if len(existing_stack) > max_len :\n",
    "        existing_stack = existing_stack[1:]\n",
    "    \n",
    "    # 4. create a numpy array\n",
    "    #np_frame = np.array(existing_stack)\n",
    "    np_frame = np.array([existing_stack[15],existing_stack[10],existing_stack[5],existing_stack[1]])\n",
    "\n",
    "    return np_frame, existing_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xRh7oFZwfu_k"
   },
   "outputs": [],
   "source": [
    "sample_frame = env.reset()\n",
    "sample_processed = process_frame(sample_frame)\n",
    "print(sample_processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zq34Hm5kAxjV"
   },
   "outputs": [],
   "source": [
    "# Create a pytorch DQN\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A-hRrOFemFsf"
   },
   "outputs": [],
   "source": [
    "# Reference for this class - https://github.com/Curt-Park/rainbow-is-all-you-need\n",
    "\n",
    "class NoisyLinear(nn.Module):\n",
    "    \"\"\"Noisy linear module for NoisyNet.\n",
    "    \n",
    "    Attributes:\n",
    "        in_features (int): input size of linear module\n",
    "        out_features (int): output size of linear module\n",
    "        std_init (float): initial std value\n",
    "        weight_mu (nn.Parameter): mean value weight parameter\n",
    "        weight_sigma (nn.Parameter): std value weight parameter\n",
    "        bias_mu (nn.Parameter): mean value bias parameter\n",
    "        bias_sigma (nn.Parameter): std value bias parameter\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, std_init: float = 0.5):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        super(NoisyLinear, self).__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.std_init = std_init\n",
    "\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(\n",
    "            torch.Tensor(out_features, in_features)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"weight_epsilon\", torch.Tensor(out_features, in_features)\n",
    "        )\n",
    "\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.register_buffer(\"bias_epsilon\", torch.Tensor(out_features))\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reset trainable network parameters (factorized gaussian noise).\"\"\"\n",
    "        mu_range = 1 / math.sqrt(self.in_features)\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(\n",
    "            self.std_init / math.sqrt(self.in_features)\n",
    "        )\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(\n",
    "            self.std_init / math.sqrt(self.out_features)\n",
    "        )\n",
    "\n",
    "    def reset_noise(self):\n",
    "        \"\"\"Make new noise.\"\"\"\n",
    "        epsilon_in = self.scale_noise(self.in_features)\n",
    "        epsilon_out = self.scale_noise(self.out_features)\n",
    "\n",
    "        # outer product\n",
    "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
    "        self.bias_epsilon.copy_(epsilon_out)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\n",
    "        \n",
    "        We don't use separate statements on train / eval mode.\n",
    "        It doesn't show remarkable difference of performance.\n",
    "        \"\"\"\n",
    "        return F.linear(\n",
    "            x,\n",
    "            self.weight_mu + self.weight_sigma * self.weight_epsilon,\n",
    "            self.bias_mu + self.bias_sigma * self.bias_epsilon,\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def scale_noise(size: int) -> torch.Tensor:\n",
    "        \"\"\"Set scale to make noise (factorized gaussian noise).\"\"\"\n",
    "        x = torch.FloatTensor(np.random.normal(loc=0.0, scale=1.0, size=size))\n",
    "\n",
    "        return x.sign().mul(x.abs().sqrt())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2089,
     "status": "error",
     "timestamp": 1588075316562,
     "user": {
      "displayName": "Krishna Naga Karthik Bodapati",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg8uqwOYuUJopIw6zGL8yskOSA2HQLIvS0U5xrX=s64",
      "userId": "08095525202426273279"
     },
     "user_tz": 240
    },
    "id": "wmH4UQpLB3wr",
    "outputId": "d29dace6-d00f-4fd1-b273-e690ad03a86d"
   },
   "outputs": [],
   "source": [
    "# This now implements the Dueling Architecture\n",
    "class DQN(nn.Module) :\n",
    "    def __init__(self,actions_size) :\n",
    "        super(DQN,self).__init__()\n",
    "\n",
    "        self.actions_size = actions_size\n",
    "        self.conv1 = nn.Conv2d(4,20,(4,4),4)\n",
    "        #torch.nn.init.xavier_normal_(self.conv1.weight)\n",
    "        self.pool1 = nn.MaxPool2d(4,1)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(20,32,(2,2),2)\n",
    "        #torch.nn.init.xavier_normal_(self.conv2.weight)\n",
    "        self.pool2 = nn.MaxPool2d((2,2),1)\n",
    "\n",
    "        # torch.Size([64, 32, 167, 143])\n",
    "        # self.linear = nn.Linear(32*11*8,512)\n",
    "        #torch.nn.init.uniform_(self.linear.weight)\n",
    "        # self.linear2 = nn.Linear(512,actions_size)\n",
    "        #torch.nn.init.uniform_(self.linear2.weight)\n",
    "\n",
    "        # Using NoisyLinear Layer instead of Linear Layer\n",
    "        self.linear = NoisyLinear(32*11*8, 512)\n",
    "        self.linear2 = NoisyLinear(512, actions_size)\n",
    "\n",
    "        self.adv_layer = nn.Linear(actions_size,actions_size)\n",
    "        self.val_layer = nn.Linear(actions_size,1)\n",
    "\n",
    "    def forward(self,x) :\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        #print(x.size())\n",
    "        x = x.view(-1,32*11*8)\n",
    "        x = self.linear(x)\n",
    "        #x = torch.tanh(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # This is the Addition of the Dueling DQN Architecture\n",
    "        advantage = self.adv_layer(x)\n",
    "\n",
    "        value = self.val_layer(x).expand(batch_size,self.actions_size)\n",
    "\n",
    "        complete_q_vals = value + advantage - advantage.mean(1).unsqueeze(1).expand(batch_size,self.actions_size)\n",
    "\n",
    "        return complete_q_vals\n",
    "\n",
    "\n",
    "    def reset_noise(self):\n",
    "        \"\"\"Reset all noisy layers.\"\"\"\n",
    "        self.linear.reset_noise()\n",
    "        self.linear2.reset_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yP-rmLs9C9fO"
   },
   "outputs": [],
   "source": [
    "# Create a buffer\n",
    "class Buffer(object) :\n",
    "    def __init__(self,max_size) :\n",
    "        self.max_size = max_size\n",
    "        self.buffer = list()\n",
    "    \n",
    "    def add(self,experience) :\n",
    "        self.buffer.append(experience)\n",
    "        #self.probabilities.append(max(self.probabilities,default=1))\n",
    "        self.resize()\n",
    "        \n",
    "    def resize(self) :\n",
    "        n = max(len(self.buffer) - self.max_size,0)\n",
    "        while n > 0 :\n",
    "            self.buffer.pop(0)\n",
    "            #self.probabilities.pop(0)\n",
    "            n -= 1\n",
    "        \n",
    "\n",
    "    def sample(self,batch_size) :\n",
    "        sample_size = min(len(self.buffer),batch_size)\n",
    "\n",
    "        random_perm_list = np.random.permutation(len(self.buffer)).tolist()\n",
    "        random_perm_list = random_perm_list[:sample_size]\n",
    "\n",
    "        return [self.buffer[i] for i in random_perm_list]\n",
    "\n",
    "    def __len__(self) :\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IElBU_rop6kT"
   },
   "outputs": [],
   "source": [
    "# Define the SumTree Class\n",
    "class SumTree(object) :\n",
    "    \"\"\"\n",
    "    Implementation of the SumTree Class\n",
    "    \"\"\"\n",
    "    def __init__(self,capacity) :\n",
    "        self.capacity = capacity # Number of leaf nodes (final nodes) that contain experience\n",
    "        self.tree = np.zeros(2*capacity - 1)\n",
    "        self.data = np.zeros(capacity,dtype=object)\n",
    "        self.data_pointer = 0\n",
    "    \n",
    "    \"\"\"\n",
    "    Add priority score in the sumTree leaf and add the experience in data\n",
    "    \"\"\"\n",
    "    def add(self,priority,data) :\n",
    "        # Look at what index we want to put the experience\n",
    "        tree_index = self.data_pointer + self.capacity - 1\n",
    "\n",
    "\n",
    "        # Update the data frame\n",
    "        self.data[self.data_pointer] = data\n",
    "\n",
    "        # Update the leaf\n",
    "        self.update(tree_index,priority)\n",
    "\n",
    "        # Add 1 to the data_pointer\n",
    "        self.data_pointer += 1\n",
    "        # If we're above the capacity, you go back to the first index (we overwrite)\n",
    "        if self.data_pointer >= self.capacity :\n",
    "            self.data_pointer = 0\n",
    "    \"\"\"\n",
    "    Update the leaf priority score and propogate the change through tree\n",
    "    \"\"\"\n",
    "    def update(self,tree_index,priority) :\n",
    "        # Change  = new priority score - former priority score\n",
    "        change = priority - self.tree[tree_index]\n",
    "        self.tree[tree_index] = priority\n",
    "\n",
    "        # then propogate the change through tree\n",
    "        while tree_index != 0 :\n",
    "            tree_index = (tree_index - 1)//2\n",
    "            self.tree[tree_index] += change\n",
    "\n",
    "    \"\"\"\n",
    "    Here we get the leaf_index, priority value of that leaf and experience associated with that index\n",
    "    \"\"\"\n",
    "    def get_leaf(self,v) :\n",
    "        parent_index = 0\n",
    "\n",
    "        while True :\n",
    "            left_child_index = 2*parent_index + 1\n",
    "            right_child_index = left_child_index + 1\n",
    "\n",
    "            # If we reach bottom, end the search\n",
    "            if left_child_index >= len(self.tree) :\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "            else :\n",
    "                # downward search, always search for a higher priority node\n",
    "                if v <= self.tree[left_child_index] :\n",
    "                    parent_index = left_child_index\n",
    "                else :\n",
    "                    v -= self.tree[left_child_index]\n",
    "                    parent_index = right_child_index\n",
    "        \n",
    "        data_index = leaf_index - self.capacity + 1\n",
    "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
    "    \n",
    "    @property\n",
    "    def total_priority(self) :\n",
    "        return self.tree[0] # returns the root node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GHjVi1iSt8wT"
   },
   "outputs": [],
   "source": [
    "class PERBuffer(object) :\n",
    "    def __init__(self,capacity) :\n",
    "        self.PER_e = 0.01\n",
    "        self.PER_a = 0.6\n",
    "        self.PER_b = 0.4\n",
    "        self.PER_b_increment_per_sampling = 0.001\n",
    "        self.absolute_error_upper = 1 # clipped abs error\n",
    "\n",
    "        # Now we make the tree\n",
    "        self.tree = SumTree(capacity)\n",
    "\n",
    "    def add(self,experience) :\n",
    "        # Find the max priority\n",
    "        max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "\n",
    "        # If the max priority = 0, we can't put priority = 0, since this will never have a change to be selected\n",
    "        # So we use a minimum priority\n",
    "        if max_priority == 0 :\n",
    "            max_priority = self.absolute_error_upper\n",
    "        \n",
    "        self.tree.add(max_priority,experience) # set the max p for new experience\n",
    "\n",
    "    def sample(self,batch_size) :\n",
    "        # Create a sample array that contains the minibatch\n",
    "        sample_experiences = list()\n",
    "\n",
    "        indexes = list()\n",
    "        importance_weights = list()\n",
    "\n",
    "        # Compute the priority\n",
    "        priority_segment = self.tree.total_priority / batch_size\n",
    "\n",
    "        # Here we increase PER_b each time a new batch is sampled\n",
    "        self.PER_b = np.min([1.,self.PER_b + self.PER_b_increment_per_sampling])\n",
    "\n",
    "        # Calculating the max_weight\n",
    "        p_min = np.min(self.tree.tree[-self.tree.capacity:])/self.tree.total_priority\n",
    "        max_weight = (p_min*batch_size) ** (-self.PER_b)\n",
    "\n",
    "        for i in range(batch_size) :\n",
    "            # Sample a value uniformly from each range\n",
    "            a, b  = priority_segment*i, priority_segment*(i + 1)\n",
    "            value = np.random.uniform(a,b)\n",
    "\n",
    "            # Experience that correspong to each value is retrieved\n",
    "            index, priority, experience = self.tree.get_leaf(value)\n",
    "\n",
    "            # P(j)\n",
    "            sample_probilities = priority / self.tree.total_priority\n",
    "\n",
    "            # Importance weights\n",
    "            importance_weight = np.power(batch_size*sample_probilities,-self.PER_b) / max_weight\n",
    "            \n",
    "            importance_weights.append(importance_weight)\n",
    "            indexes.append(index)\n",
    "            sample_experiences.append(experience)\n",
    "\n",
    "        #print(len(indexes))\n",
    "        #print(len(importance_weights))\n",
    "        return sample_experiences, indexes, np.array(importance_weights)\n",
    "    \n",
    "    def set_priorities(self,indexes,errors) :\n",
    "        #print(errors.shape)\n",
    "        #print(len(indexes))\n",
    "        for i in range(len(indexes)) :\n",
    "            index = indexes[i]\n",
    "            error = errors[i]\n",
    "            self.tree.update(index,error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sWNmFVYBFol8"
   },
   "outputs": [],
   "source": [
    "# Create a buffer\n",
    "class PERBuffer_OLD :\n",
    "    def __init__(self,max_size) :\n",
    "        self.max_size = max_size\n",
    "        self.buffer = list()\n",
    "        self.probabilities = list()\n",
    "    \n",
    "    def add(self,experience) :\n",
    "        self.buffer.append(experience)\n",
    "        self.probabilities.append(max(self.probabilities,default=1))\n",
    "        #print(len(self.probabilities))\n",
    "        self.resize()\n",
    "        \n",
    "    def resize(self) :\n",
    "        n = max(len(self.buffer) - self.max_size,0)\n",
    "        while n > 0 :\n",
    "            self.buffer.pop(0)\n",
    "            self.probabilities.pop(0)\n",
    "            n -= 1\n",
    "\n",
    "    def get_proabilities(self,alpha=1) :\n",
    "        np_probs = np.array(self.probabilities) ** alpha\n",
    "        sample_probs = np_probs / np_probs.sum()\n",
    "        #print(sample_probs.sum())\n",
    "        return sample_probs\n",
    "\n",
    "    def get_importance(self,sample_probs,indexes,beta=1) :\n",
    "        #N = len(self.buffer)\n",
    "        N = self.max_size # right now instead of N being a variying number, I am using a constant N\n",
    "        importance_weights = list()\n",
    "        for i in indexes :\n",
    "            importance_weights.append(1.0/sample_probs[i])\n",
    "        \n",
    "        np_importance = np.array(importance_weights)/N\n",
    "        np_importance_normalized = (np_importance ** beta) / np.max(np_importance)\n",
    "        return np_importance_normalized\n",
    "\n",
    "    def set_priorities(self,indexes,error) :\n",
    "        offset = 0.1\n",
    "        #print(error.shape)\n",
    "        for i in range(len(indexes)) :\n",
    "            self.probabilities[indexes[i]] = error[i] + offset\n",
    "\n",
    "    def sample(self,batch_size) :\n",
    "        sample_size = min(len(self.buffer),batch_size)\n",
    "        sample_probs = self.get_proabilities(1)\n",
    "\n",
    "        #random_perm_list = np.random.permutation(len(self.buffer)).tolist()\n",
    "        random_perm_list = np.random.choice(np.arange(len(self.buffer)),size=sample_size,p=sample_probs).tolist()\n",
    "        #random_perm_list = random_perm_list[:sample_size]\n",
    "        importance = self.get_importance(sample_probs,random_perm_list,1)\n",
    "\n",
    "        return [self.buffer[i] for i in random_perm_list], random_perm_list, importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vR6bjHw7GVvv"
   },
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dU9kb-QTO7K6"
   },
   "outputs": [],
   "source": [
    "# Create an empty stack\n",
    "def create_new_stack(name_env) :\n",
    "    new_stack = list()\n",
    "    for _ in range(16) :\n",
    "        new_stack.append(np.zeros((110,84),dtype=np.int))\n",
    "    return new_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8fcW3pxC3Acg"
   },
   "outputs": [],
   "source": [
    "# Fill up the buffer\n",
    "def fill_up_buffer(num_exp) :\n",
    "    existing_stack = create_new_stack('SpaceInvaders-v0')\n",
    "    # collect all examples randomly\n",
    "    state = env.reset()\n",
    "    state, existing_stack = create_frame_stack(state,existing_stack)\n",
    "\n",
    "    step = 0\n",
    "    prev_action = action_space.sample()\n",
    "\n",
    "    num_lives = env.ale.lives()\n",
    "    for _ in range(num_exp) :\n",
    "\n",
    "        if step % 4 == 0 :\n",
    "            action = action_space.sample()\n",
    "        else :\n",
    "            action = prev_action\n",
    "\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        lives_remaining = env.ale.lives()\n",
    "        if lives_remaining < num_lives :\n",
    "          done = True\n",
    "          num_lives = lives_remaining\n",
    "\n",
    "        # Introduce Reward Clipping here also\n",
    "        reward = max(reward,1) # get a positive reward if survived\n",
    "        reward = max(reward,4) # Get additional points for a hit\n",
    "\n",
    "        if done :\n",
    "            next_state = np.zeros((210,160,3),dtype=np.int)\n",
    "            next_state, existing_stack = create_frame_stack(next_state,existing_stack)\n",
    "\n",
    "            #print(\"Reward when done : \",reward)\n",
    "\n",
    "            if step % 13 == 0 :\n",
    "                existing_buffer.add((state,action,-2,next_state,done))\n",
    "                per_existing_buffer.add((state,action,-2,next_state,done))\n",
    "        else :\n",
    "            next_state, existing_stack = create_frame_stack(next_state,existing_stack)\n",
    "            if step % 13 == 0 :\n",
    "                existing_buffer.add((state,action,reward,next_state,done))\n",
    "                per_existing_buffer.add((state,action,reward,next_state,done))\n",
    "            state = next_state\n",
    "            #print(\"Reward in normal \",reward)\n",
    "        \n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wdRXiiItcW_l"
   },
   "outputs": [],
   "source": [
    "# This is train function with Dueling DDQN\n",
    "def train(batch_size,gamma) :\n",
    "    # This is the train loop, where I combine all the learning steps\n",
    "    # 1. get all lists\n",
    "    states_list = list()\n",
    "    actions_list = list()\n",
    "    rewards_list = list()\n",
    "    next_states_list = list()\n",
    "    dones_list = list()\n",
    "    batch = existing_buffer.sample(batch_size)\n",
    "    for item in batch :\n",
    "        state, action, reward, next_state, done = item\n",
    "        states_list.append(state)\n",
    "        actions_list.append(action)\n",
    "        rewards_list.append(reward)\n",
    "        next_states_list.append(next_state)\n",
    "        if done :\n",
    "            dones_list.append(0)\n",
    "        else :\n",
    "            dones_list.append(1)\n",
    "    \n",
    "    np_states = np.asarray(states_list)\n",
    "    np_actions = np.asarray(actions_list)\n",
    "    np_next_states = np.asarray(next_states_list)\n",
    "    np_dones = np.asarray(dones_list)\n",
    "    np_rewards = np.asarray(rewards_list)\n",
    "\n",
    "    torch_states = Variable(torch.from_numpy(np_states)).type(torch.float)\n",
    "    torch_actions = Variable(torch.from_numpy(np_actions)).type(torch.LongTensor)\n",
    "    torch_next_state = Variable(torch.from_numpy(np_next_states)).type(torch.float)\n",
    "    torch_dones = Variable(torch.from_numpy(np_dones)).type(torch.long)\n",
    "    torch_rewards = Variable(torch.from_numpy(np_rewards)).type(torch.long)\n",
    "\n",
    "    # transfer tensors to cuda\n",
    "    torch_states = torch_states.cuda()\n",
    "    torch_actions = torch_actions.cuda()\n",
    "    torch_next_state = torch_next_state.cuda()\n",
    "    torch_dones = torch_dones.cuda()\n",
    "    torch_rewards = torch_rewards.cuda()\n",
    "\n",
    "    all_q_vals = dqn(torch_states)\n",
    "    pred_q_val = all_q_vals.gather(1,torch_actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    # below should be a target dqn, next q vals are estimated using our own DQN, and also we compute \n",
    "    with torch.no_grad() :\n",
    "        next_state_q_vals = target_dqn(torch_next_state)\n",
    "        next_state_target_q_vals = dqn(torch_next_state)\n",
    "\n",
    "\n",
    "    # Next state actions are estimated using our dqn whilst the next value of those actions are estimated using our ddqn  \n",
    "    next_state_q_actions = next_state_q_vals.max(1)[1]\n",
    "    next_state_q_val = next_state_target_q_vals.gather(1,next_state_q_actions.unsqueeze(1)).squeeze(1)\n",
    "    \n",
    "    #next_state_q_val = next_state_q_vals.max(1)[0]\n",
    "    #next_state_q_val = next_state_q_vals.gather(1,next_state_q_vals.max(1)[1].unsqueeze(1)).squeeze(1)\n",
    "    expected_q_val = torch_rewards + gamma * next_state_q_val * torch_dones\n",
    "    \n",
    "\n",
    "    # now form the targets\n",
    "    #targets = list()\n",
    "    #for i in range(len(dones_list)) :\n",
    "    #    if dones_list[i] :\n",
    "    #        targets.append([rewards_list[i]])\n",
    "    #    else :\n",
    "    #        targets.append([rewards_list[i] + gamma*max_q_list[i]])\n",
    "    #\n",
    "    #np_targets = np.array(targets)\n",
    "    #torch_targets = torch.from_numpy(np_targets)\n",
    "\n",
    "    #torch_targets = torch_targets.cuda()\n",
    "    #print(torch_targets.size())\n",
    "\n",
    "    \n",
    "    #loss = (pred_q_val - expected_q_val.detach()).pow(2).mean()\n",
    "    loss = criterion(pred_q_val,expected_q_val.detach()).mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    dqn.reset_noise()\n",
    "    target_dqn.reset_noise()\n",
    "\n",
    "\n",
    "\n",
    "    for param in dqn.parameters() :\n",
    "        param.grad.data.clamp_(-1,1)\n",
    "    optimizer.step()\n",
    "\n",
    "    # delete what is useless\n",
    "    del states_list, actions_list, rewards_list, next_states_list, dones_list, batch, np_states, np_next_states, np_actions, np_dones, np_rewards\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dDVai9UxKiDK"
   },
   "outputs": [],
   "source": [
    "def per_train(batch_size,gamma) :\n",
    "    # This is the train loop, where I combine all the learning steps\n",
    "    # 1. get all lists\n",
    "    states_list = list()\n",
    "    actions_list = list()\n",
    "    rewards_list = list()\n",
    "    next_states_list = list()\n",
    "    dones_list = list()\n",
    "    batch, indexes, importance = per_existing_buffer.sample(batch_size)\n",
    "\n",
    "    #print(\"----BATCH_SIZE\")\n",
    "    #print(len(indexes))\n",
    "    #print(len(batch))\n",
    "    for item in batch :\n",
    "        state, action, reward, next_state, done = item\n",
    "        states_list.append(state)\n",
    "        actions_list.append(action)\n",
    "        rewards_list.append(reward)\n",
    "        next_states_list.append(next_state)\n",
    "        if done :\n",
    "            dones_list.append(0)\n",
    "        else :\n",
    "            dones_list.append(1)\n",
    "    \n",
    "    np_states = np.asarray(states_list)\n",
    "    np_actions = np.asarray(actions_list)\n",
    "    np_next_states = np.asarray(next_states_list)\n",
    "    np_dones = np.asarray(dones_list)\n",
    "    np_rewards = np.asarray(rewards_list)\n",
    "    \n",
    "    torch_states = Variable(torch.from_numpy(np_states)).type(torch.float)\n",
    "    torch_actions = Variable(torch.from_numpy(np_actions)).type(torch.LongTensor)\n",
    "    torch_next_state = Variable(torch.from_numpy(np_next_states)).type(torch.float)\n",
    "    torch_dones = Variable(torch.from_numpy(np_dones)).type(torch.long)\n",
    "    torch_rewards = Variable(torch.from_numpy(np_rewards)).type(torch.long)\n",
    "\n",
    "    # transfer tensors to cuda\n",
    "    torch_states = torch_states.cuda()\n",
    "    torch_actions = torch_actions.cuda()\n",
    "    torch_next_state = torch_next_state.cuda()\n",
    "    torch_dones = torch_dones.cuda()\n",
    "    torch_rewards = torch_rewards.cuda()\n",
    "\n",
    "    all_q_vals = per_dqn(torch_states)\n",
    "    pred_q_val = all_q_vals.gather(1,torch_actions.unsqueeze(1)).squeeze(1)\n",
    "    \n",
    "    # below are the changes for PER + Dueling + Double DQN\n",
    "    #with torch.no_grad() :\n",
    "    #    next_state_q_vals = per_target_dqn(torch_next_state.float())\n",
    "        \n",
    "    #next_state_q_val = next_state_q_vals.max(1)[0]\n",
    "    ##next_state_q_val = next_state_q_vals.gather(1,next_state_q_vals.max(1)[1].unsqueeze(1)).squeeze(1)\n",
    "    # below should be a target dqn, next q vals are estimated using our own DQN, and also we compute\n",
    "    with torch.no_grad() :\n",
    "        next_state_q_vals = per_target_dqn(torch_next_state)\n",
    "        next_state_target_q_vals = per_dqn(torch_next_state)\n",
    "\n",
    "    # Next state actions are estimated using our dqn whilst the next value of those actions are estimated using our ddqn  \n",
    "    next_state_q_actions = next_state_q_vals.max(1)[1]\n",
    "    next_state_q_val = next_state_target_q_vals.gather(1,next_state_q_actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    expected_q_val = torch_rewards + gamma * next_state_q_val * torch_dones\n",
    "\n",
    "    #print(\"--Init_Compute---\")\n",
    "    #print(expected_q_val.size())\n",
    "    #print(pred_q_val.size())\n",
    "    '''max_q_vals = next_state_q_vals.max(1)[0]\n",
    "    max_q_list = max_q_vals.cpu().detach().numpy().tolist()\n",
    "\n",
    "    # now form the targets\n",
    "    targets = list()\n",
    "    for i in range(len(dones_list)) :\n",
    "        if dones_list[i] :\n",
    "            targets.append([rewards_list[i]])\n",
    "        else :\n",
    "            targets.append([rewards_list[i] + gamma*max_q_list[i]])\n",
    "    \n",
    "    np_targets = np.array(targets)\n",
    "    torch_targets = torch.from_numpy(np_targets)\n",
    "\n",
    "    torch_targets = torch_targets.cuda()\n",
    "    #print(torch_targets.size())'''\n",
    "    #torch_importance = Variable(torch.from_numpy(importance)).type(torch.float) #** 2\n",
    "    #torch_importance = Variable(torch.from_numpy(importance)\n",
    "    torch_importance = torch.from_numpy(importance)\n",
    "    torch_importance = torch_importance.cuda()\n",
    "\n",
    "    per_optimizer.zero_grad()\n",
    "    #loss = per_criterion(torch_targets * torch_importance,pred_q_val * torch_importance)\n",
    "    loss_vec = torch_importance*per_criterion(pred_q_val,expected_q_val.detach())\n",
    "    #print(\"----LOSS_VEC_SHAPE----\")\n",
    "    #print(loss_vec.size())\n",
    "    loss = loss_vec.mean()\n",
    "    loss.backward()\n",
    "\n",
    "    per_dqn.reset_noise()\n",
    "    per_target_dqn.reset_noise()\n",
    "\n",
    "\n",
    "\n",
    "    for param in per_dqn.parameters() :\n",
    "        param.grad.data.clamp_(-1,1)\n",
    "        \n",
    "    per_optimizer.step()\n",
    "\n",
    "    # compute the TD-Error\n",
    "    #print(\"---TD SIZES----\")\n",
    "    #print(expected_q_val.size())\n",
    "    #print(pred_q_val.size())\n",
    "\n",
    "    error = abs(expected_q_val.cpu().detach().numpy() - pred_q_val.cpu().detach().numpy())\n",
    "\n",
    "    #error = error / np.max(error)\n",
    "    #print(\"Shape of error\")\n",
    "    #print(error.shape)\n",
    "    per_existing_buffer.set_priorities(indexes,error)\n",
    "\n",
    "    # delete what is useless\n",
    "    del states_list, actions_list, rewards_list, next_states_list, dones_list, batch, np_states, np_next_states, np_actions, np_dones, np_rewards\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Od6dyTHwknyT"
   },
   "outputs": [],
   "source": [
    "def take_action(state) :\n",
    "    #print(state.shape)\n",
    "\n",
    "\n",
    "    #print(\"Policy Action Taken\")\n",
    "    batch_state = np.array([state])\n",
    "    torch_state = Variable(torch.from_numpy(batch_state)).type(torch.float)\n",
    "    torch_state = torch_state.cuda()\n",
    "    with torch.no_grad() :\n",
    "        q_vals = dqn(torch_state)\n",
    "    \n",
    "    action = q_vals.max(1)[1].item()\n",
    "        #action = action_tensor.item()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PB6zEl1TNkPn"
   },
   "outputs": [],
   "source": [
    "def per_take_action(state) :\n",
    "    #print(state.shape)\n",
    "    #print(\"Policy Action Taken\")\n",
    "    batch_state = np.array([state])\n",
    "    torch_state = torch.from_numpy(batch_state)\n",
    "    torch_state = torch_state.cuda()\n",
    "    with torch.no_grad() :\n",
    "        q_vals = per_dqn(torch_state.float())\n",
    "    \n",
    "    action_tensor = q_vals.max(1)[1]\n",
    "    action = action_tensor.item()\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CfldLbyRid0i"
   },
   "outputs": [],
   "source": [
    "# Time to collect the buffer\n",
    "def collect_experience() :\n",
    "    # The goal is to add more experience to this buffer\n",
    "    existing_stack = create_new_stack('SpaceInvaders-v0')\n",
    "    state = env.reset()\n",
    "    state, existing_stack = create_frame_stack(state,existing_stack)\n",
    "    #print(state.shape)\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    prev_action = action_space.sample()\n",
    "    loss = 0.0\n",
    "    num_lives = env.ale.lives()\n",
    "    #max_lives = env.ale.lives()\n",
    "    loss = 0.0\n",
    "    train_step = 0\n",
    "    while num_lives > 0 :\n",
    "        if step % 4 == 0:\n",
    "            action = take_action(state)\n",
    "\n",
    "            #if step % 1000 == 0 :\n",
    "            #  target_dqn.load_state_dict(dqn.state_dict())\n",
    "\n",
    "        else :\n",
    "            action = prev_action\n",
    "\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        lives_remaining = env.ale.lives()\n",
    "        if lives_remaining < num_lives :\n",
    "          done = True\n",
    "          num_lives = lives_remaining\n",
    "          life_lost = True\n",
    "\n",
    "        prev_action = action\n",
    "        total_reward += reward\n",
    "\n",
    "        # Introducing Reward Clipping\n",
    "        reward = max(reward,1) # Atleast one point if not died\n",
    "        reward = min(reward,4) # Additional 3 points for every hit\n",
    "\n",
    "        if done :\n",
    "            terminal_state = np.zeros((210,160,3),dtype=np.int)\n",
    "            terminal_state, existing_stack = create_frame_stack(terminal_state,existing_stack)\n",
    "\n",
    "            if step % 13 == 0 :\n",
    "                existing_buffer.add((state,action,-2,terminal_state,done))\n",
    "            #break\n",
    "\n",
    "            t_loss = 0.0\n",
    "            for _ in range(10) :\n",
    "              t_loss += train(32,0.95)\n",
    "            \n",
    "            t_loss /= 10\n",
    "            loss += t_loss\n",
    "            train_step += 1\n",
    "\n",
    "            done = False\n",
    "            existing_stack = create_new_stack('SpaceInvaders-v0')\n",
    "            next_state, existing_stack = create_frame_stack(next_state,existing_stack)\n",
    "            state = next_state\n",
    "\n",
    "        else :\n",
    "            next_state, existing_stack = create_frame_stack(next_state,existing_stack)\n",
    "\n",
    "            if step % 13 == 0 :\n",
    "                existing_buffer.add((state,action,reward,next_state,done))\n",
    "\n",
    "            state = next_state\n",
    "        step += 1\n",
    "\n",
    "    return total_reward, loss/train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UmbZ4ptUob4m"
   },
   "outputs": [],
   "source": [
    "# Buffer but with Prioritized Experience Replay\n",
    "def per_collect_experience() :\n",
    "    # The goal is to add more experience to this buffer\n",
    "    existing_stack = create_new_stack('SpaceInvaders-v0')\n",
    "    state = env.reset()\n",
    "    state, existing_stack = create_frame_stack(state,existing_stack)\n",
    "    #print(state.shape)\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    prev_action = action_space.sample()\n",
    "    loss = 0.0\n",
    "    num_lives = env.ale.lives()\n",
    "    loss = 0.0\n",
    "    train_step = 0\n",
    "    while num_lives > 0 :\n",
    "        if step % 4 == 0:\n",
    "            action = per_take_action(state)\n",
    "            #loss += per_train(64,0.95)\n",
    "            #train_step += 1\n",
    "            #if step % 10000 == 0 :\n",
    "            #  per_target_dqn.load_state_dict(per_dqn.state_dict())\n",
    "            \n",
    "        else :\n",
    "            action = prev_action\n",
    "\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        lives_remaining = env.ale.lives()\n",
    "        if lives_remaining < num_lives :\n",
    "          done = True\n",
    "          num_lives = lives_remaining\n",
    "\n",
    "        prev_action = action\n",
    "        total_reward += reward\n",
    "\n",
    "        # Introducing Reward Clipping\n",
    "        reward = max(reward,1) # Atleast one point if not died\n",
    "        reward = min(reward,4) # Additional 3 points for every hit\n",
    "\n",
    "        if done :\n",
    "            terminal_state = np.zeros((210,160,3),dtype=np.int)\n",
    "            terminal_state, existing_stack = create_frame_stack(terminal_state,existing_stack)\n",
    "\n",
    "            if step % 13 == 0 :\n",
    "                per_existing_buffer.add((state,action,-2,terminal_state,done))\n",
    "            #break\n",
    "\n",
    "            t_loss = 0.0\n",
    "            for _ in range(10) :\n",
    "              t_loss += per_train(32,0.95)\n",
    "\n",
    "            t_loss /= 10\n",
    "            loss += t_loss\n",
    "            train_step += 1\n",
    "\n",
    "            done = False\n",
    "            existing_stack = create_new_stack('SpaceInvaders-v0')\n",
    "            next_state, existing_stack = create_frame_stack(next_state,existing_stack)\n",
    "            state = next_state\n",
    "\n",
    "        else :\n",
    "            next_state, existing_stack = create_frame_stack(next_state,existing_stack)\n",
    "\n",
    "            if step % 13 == 0 :\n",
    "                per_existing_buffer.add((state,action,reward,next_state,done))\n",
    "\n",
    "            state = next_state\n",
    "        step += 1\n",
    "\n",
    "    return total_reward, loss/train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sbIGcC6UOu_m"
   },
   "outputs": [],
   "source": [
    "existing_buffer = Buffer(10000)\n",
    "per_existing_buffer = PERBuffer(10000)\n",
    "fill_up_buffer(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BV3sS2Dc2zhK"
   },
   "outputs": [],
   "source": [
    "# Here are the steps we need to follow\n",
    "# 1. Create a buffer\n",
    "# 2. Fill it up\n",
    "# 3. for episode in episodes\n",
    "#   3a Collect experience\n",
    "#   3b Learning from those experience\n",
    "\n",
    "total_rewards = list()\n",
    "\n",
    "decay_factor = 0.995\n",
    "dqn = DQN(action_space.n)\n",
    "dqn = dqn.cuda()\n",
    "target_dqn = DQN(action_space.n)\n",
    "target_dqn = target_dqn.cuda()\n",
    "target_dqn.load_state_dict(dqn.state_dict())\n",
    "\n",
    "criterion = F.smooth_l1_loss\n",
    "optimizer = optim.Adam(dqn.parameters(),lr=1e-6)\n",
    "update_interval = 8\n",
    "tau = 1e-3\n",
    "\n",
    "\n",
    "max_collected_reward = 0.0\n",
    "batch_updates = 20\n",
    "\n",
    "loss_list = list()\n",
    "\n",
    "\n",
    "loss = 0.0\n",
    "running_average = list()\n",
    "for episode in range(500) :\n",
    "\n",
    "    total_reward, loss = collect_experience()\n",
    "\n",
    "    running_average.append(total_reward)\n",
    "    \n",
    "    if len(running_average) > 10 :\n",
    "        running_average.pop(0)\n",
    "    #total_reward /= 10\n",
    "    print(\"---------------------------------\")\n",
    "    print(\"Episode :\",episode)\n",
    "    print(\"Loss : \",loss)\n",
    "    print(\"Total Reward :\",total_reward)\n",
    "    print(\"Max Rewards Seen Untill Now :\",max_collected_reward)\n",
    "    print(\"Length of Buffer is :\",len(existing_buffer.buffer))\n",
    "\n",
    "    print(\"Running Average is :\",sum(running_average)/10)\n",
    "    print(\"-----------------------------\")\n",
    "    loss_list.append(loss)\n",
    "\n",
    "    #print(len(existing_buffer.buffer))\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "\n",
    "    if total_reward > max_collected_reward :\n",
    "        max_collected_reward = total_reward\n",
    "        #target_dqn.load_state_dict(dqn.state_dict())\n",
    "        torch.save(dqn.state_dict(),'best_rl_model.pt')\n",
    "\n",
    "    # Introducing soft-update\n",
    "    if episode % update_interval :\n",
    "      for target_dqn_param, dqn_param in zip(target_dqn.parameters(),dqn.parameters()) :\n",
    "        target_dqn_param.data.copy_(tau*dqn_param.data + (1.0-tau)*target_dqn_param.data)\n",
    "    #if episode % update_interval == 0 :\n",
    "    #    target_dqn.load_state_dict(dqn.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KVu48uhOH9dz"
   },
   "outputs": [],
   "source": [
    "window = 100\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward (SMA 100)')\n",
    "plt.plot([np.mean(total_rewards[tr:tr+window]) for tr in range(window, len(total_rewards)-window)],'b',label='NoPER')\n",
    "plt.legend(loc='upper left')\n",
    "plt.savefig('RewardChartNoPERSpaceInvaders.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZoT3hQE9OkB3"
   },
   "outputs": [],
   "source": [
    "# THIS IS THE PRIORITIZED EXPERIENCE REPLAY TRAIN LOOP\n",
    "per_total_rewards = list()\n",
    "\n",
    "decay_factor = 0.995\n",
    "per_dqn = DQN(action_space.n)\n",
    "per_dqn = per_dqn.cuda()\n",
    "\n",
    "per_target_dqn = DQN(action_space.n)\n",
    "per_target_dqn = per_target_dqn.cuda()\n",
    "\n",
    "per_target_dqn.load_state_dict(per_dqn.state_dict())\n",
    "\n",
    "per_criterion = F.smooth_l1_loss\n",
    "#per_optimizer = optim.SGD(per_dqn.parameters(),lr=0.01,momentum=0.9)\n",
    "per_optimizer = optim.Adam(per_dqn.parameters(),lr=1e-6)\n",
    "update_interval = 8\n",
    "tau = 1e-3\n",
    "per_max_collected_reward = 0.0\n",
    "batch_updates = 50\n",
    "\n",
    "per_loss_list = list()\n",
    "\n",
    "running_average = list()\n",
    "\n",
    "for episode in range(500) :\n",
    "\n",
    "    total_reward, loss = per_collect_experience()\n",
    "\n",
    "    running_average.append(total_reward)\n",
    "    \n",
    "    if len(running_average) > 10 :\n",
    "        running_average.pop(0)\n",
    "    #total_reward /= 10\n",
    "    print(\"------------PER TRAIN LOOP---------------------\")\n",
    "    print(\"Episode :\",episode)\n",
    "    print(\"Loss : \",loss)\n",
    "    print(\"Total Reward :\",total_reward)\n",
    "    print(\"Max Rewards Seen Untill Now :\",per_max_collected_reward)\n",
    "    print(\"Length of Buffer is :\",per_existing_buffer.tree.data_pointer)\n",
    "\n",
    "    print(\"Running Average is :\",sum(running_average)/10)\n",
    "    print(\"-----------------------------\")\n",
    "    per_loss_list.append(loss)\n",
    "\n",
    "    #print(len(existing_buffer.buffer))\n",
    "    per_total_rewards.append(total_reward)\n",
    "\n",
    "\n",
    "    if total_reward > per_max_collected_reward :\n",
    "        per_max_collected_reward = total_reward\n",
    "        #per_target_dqn.load_state_dict(per_dqn.state_dict())\n",
    "        torch.save(per_dqn.state_dict(),'best_per_rl_model.pt')\n",
    "\n",
    "    # Introducing soft-update\n",
    "    if episode % update_interval :\n",
    "      for per_target_dqn_param, per_dqn_param in zip(per_target_dqn.parameters(),per_dqn.parameters()) :\n",
    "        per_target_dqn_param.data.copy_(tau*per_dqn_param.data + (1.0-tau)*per_target_dqn_param.data)\n",
    "\n",
    "    #if episode % update_interval == 0 :\n",
    "    #    per_target_dqn.load_state_dict(per_dqn.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gezXPkEF-sIr"
   },
   "outputs": [],
   "source": [
    "window = 100\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward (SMA 100)')\n",
    "plt.plot([np.mean(per_total_rewards[tr:tr+window]) for tr in range(window, len(per_total_rewards)-window)],'b',label='With PER Dueling DDQN')\n",
    "plt.legend(loc='upper left')\n",
    "plt.savefig('RewardChartPERSpaceInvaders.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ExywvfDxv65v"
   },
   "outputs": [],
   "source": [
    "window = 100\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Rewards (SMA 100)')\n",
    "plt.plot([np.mean(total_rewards[tr:tr+window]) for tr in range(window, len(total_rewards) - window)],'b',label='Dueling DQN')\n",
    "plt.plot([np.mean(per_total_rewards[tr:tr+window]) for tr in range(window, len(per_total_rewards) - window)],'r',label='Dueling DQN with Prioritized Experience Replay')\n",
    "plt.legend(loc='upper left')\n",
    "plt.savefig('RewardChartWithPERSpaceInvaders.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l3jFO6Wqacm4"
   },
   "outputs": [],
   "source": [
    "# This is the next step in evolution for the thPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fx03z-2Q_ni0"
   },
   "outputs": [],
   "source": [
    "window = 10\n",
    "plt.plot([np.mean(loss_list[tr:tr+window]) for tr in range(window, len(loss_list) - window)],label='Loss for Dueling Double DQN')\n",
    "plt.plot([np.mean(per_loss_list[tr:tr+window]) for tr in range(window, len(per_loss_list) - window)],label='Loss for Dueling Double DQN with PER')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wev5vp9y_qUt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "RL_Assignment2_SpaceInvaders_DuelingDDQN_PER_Noisy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
